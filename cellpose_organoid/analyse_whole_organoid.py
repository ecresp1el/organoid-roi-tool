#!/usr/bin/env python3
"""
Summarise manually curated Cellpose whole-organoid masks (``*_seg.npy``).

This helper ingests the metadata produced by
``prepare_for_cellprofiler_cellpose.py`` and the multi-channel TIFF stacks
under ``cellpose_multichannel_zcyx``. For every TIFF it looks for the matching
``*_seg.npy`` file generated by the Cellpose GUI, calculates per-image
statistics inside the mask (pixel count, mean/median intensity, etc.), and
recreates the WT vs KO summary tables/plots used in the projection analyses.

Example:

    conda activate organoid_roi_incucyte_imaging
    python cellpose_organoid/analyse_whole_organoid.py \\
        --base-path /Volumes/Manny4TBUM/10_16_2025/lhx6_pdch19_WTvsKO_projectfolder \\
        --analysis PCDHvsLHX6_WTvsKO_IHC \\
        --projection max
"""

from __future__ import annotations

import argparse
import warnings
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, List, Optional, Sequence

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tifffile as tiff

try:
    from scipy.stats import mannwhitneyu, ttest_ind  # type: ignore
except Exception as exc:  # pragma: no cover - environment dependent
    raise RuntimeError(
        "scipy.stats (mannwhitneyu/ttest_ind) is required. Install scipy in the active environment."
    ) from exc


DEFAULT_PROJECTIONS = ("max", "mean", "median")
GROUP_COLORS = {"WT": "#1f77b4", "KO": "#d62728"}
CONFIDENCE_Z = 1.96  # 95% CI


@dataclass
class ChannelMeta:
    slug: str
    canonical: str
    marker: str
    wavelength_nm: Optional[float]


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Analyse Cellpose whole-organoid *_seg.npy masks.")
    parser.add_argument("--base-path", type=Path, required=True, help="Project folder used by the pipeline.")
    parser.add_argument(
        "--analysis",
        required=True,
        help="Analysis name (e.g. PCDHvsLHX6_WTvsKO_IHC or NestinvsDcx_WTvsKO_IHC).",
    )
    parser.add_argument(
        "--projection",
        dest="projections",
        action="append",
        default=None,
        help="Projection type(s) to include (max, mean, median). Repeat for multiple.",
    )
    parser.add_argument(
        "--group",
        dest="groups",
        action="append",
        default=None,
        help="Optional subset of experimental groups (e.g. WT, KO).",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=None,
        help="Override the default output directory "
        "(defaults to <base>/analysis_results/<analysis>/whole_organoid_analysis).",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    base_path = args.base_path.expanduser().resolve()
    if not base_path.exists():
        raise FileNotFoundError(f"Base path not found: {base_path}")

    projections = tuple(args.projections) if args.projections else DEFAULT_PROJECTIONS
    groups = {grp.upper() for grp in args.groups} if args.groups else None

    output_root = (
        args.output_dir.expanduser().resolve()
        if args.output_dir
        else base_path / "analysis_results" / args.analysis / "whole_organoid_analysis"
    )
    data_dir = output_root / "analysis_pipeline"

    paths = discover_metadata_paths(base_path)
    multi_df = load_csv(paths.multi)
    single_df = load_csv(paths.single)
    if multi_df.empty:
        raise FileNotFoundError(f"No multi-channel metadata found at {paths.multi}")

    # Filter metadata
    subset = multi_df[multi_df["analysis"] == args.analysis]
    if projections:
        subset = subset[subset["projection_type"].isin(projections)]
    if groups:
        subset = subset[subset["group"].str.upper().isin(groups)]
    if subset.empty:
        raise RuntimeError("No metadata rows match the provided filters.")

    single_lookup = single_df.set_index("export_path") if not single_df.empty else None
    records = analyse_rows(subset, single_lookup)
    if not records:
        raise RuntimeError("No measurements were produced (missing masks?).")

    results = pd.DataFrame(records)
    results.sort_values(["channel_slug", "projection_type", "group", "sample_id"], inplace=True)

    output_root.mkdir(parents=True, exist_ok=True)
    (output_root / "whole_organoid_results.csv").write_text(results.to_csv(index=False))

    for channel_slug, channel_df in results.groupby("channel_slug"):
        save_channel_outputs(channel_slug, channel_df, data_dir)

    print(f"[DONE] Whole-organoid analysis complete. Outputs in {output_root}")


def discover_metadata_paths(base_path: Path):
    multi = base_path / "cellprofilerandcellpose_folder" / "cellpose_multichannel_zcyx" / "cellpose_multichannel_metadata.csv"
    single = base_path / "cellprofilerandcellpose_folder" / "cellprofilerandcellpose_metadata.csv"
    return argparse.Namespace(multi=multi, single=single)


def load_csv(path: Path) -> pd.DataFrame:
    if not path.exists():
        return pd.DataFrame()
    return pd.read_csv(path)


def analyse_rows(subset: pd.DataFrame, single_lookup: Optional[pd.DataFrame]) -> List[dict]:
    records: List[dict] = []
    for row in subset.itertuples():
        export_path = Path(row.export_path)
        if not export_path.exists():
            print(f"[WARN] Skipping missing TIFF: {export_path}")
            continue
        mask_path = export_path.with_name(export_path.stem + "_seg.npy")
        if not mask_path.exists():
            print(f"[WARN] Missing mask for {export_path.name}: {mask_path}")
            continue

        try:
            stack = tiff.imread(export_path)
        except Exception as exc:
            print(f"[WARN] Failed to read {export_path}: {exc}")
            continue

        data = ensure_channel_first(stack, int(row.channel_count))

        try:
            mask = np.load(mask_path, allow_pickle=True)
        except Exception as exc:
            print(f"[WARN] Failed to load mask {mask_path}: {exc}")
            continue

        mask_bool = extract_mask_array(mask)
        if mask_bool.ndim != 2:
            print(f"[WARN] Unexpected mask shape {mask_bool.shape} for {mask_path}; skipping.")
            continue
        pixel_count = float(mask_bool.sum())
        if pixel_count == 0:
            print(f"[WARN] Empty mask for {export_path}; skipping.")
            continue

        if hasattr(row, "source_exports") and pd.notna(row.source_exports):
            source_exports = [path.strip() for path in str(row.source_exports).split("|") if path.strip()]
        else:
            source_exports = []

        channels_value = row.channels if hasattr(row, "channels") else ""
        if isinstance(channels_value, str):
            channel_labels = [part.strip() for part in channels_value.split("|") if part.strip()]
        else:
            channel_labels = []
        if not channel_labels or len(channel_labels) < data.shape[0]:
            # fall back to index-based labels
            channel_labels = channel_labels + [f"channel_{i}" for i in range(len(channel_labels), data.shape[0])]

        for idx, src in enumerate(source_exports):
            if idx >= data.shape[0]:
                print(f"[WARN] Stack channel count mismatch for {export_path}; skipping channel index {idx}.")
                continue
            channel_data = data[idx].astype(np.float64, copy=False)
            masked_pixels = channel_data[mask_bool]
            if masked_pixels.size == 0:
                continue
            stats_dict = compute_statistics(masked_pixels, pixel_count)
            meta = lookup_channel_metadata(src, channel_labels, idx, single_lookup)
            record = {
                "analysis": row.analysis,
                "group": row.group,
                "sample_id": row.sample_id,
                "projection_type": row.projection_type,
                "filename": Path(row.export_path).name,
                "export_path": str(export_path),
                "mask_path": str(mask_path),
                "channel_index": idx,
                "channel_slug": meta.slug,
                "channel_canonical": meta.canonical,
                "channel_marker": meta.marker,
                "channel_wavelength_nm": meta.wavelength_nm,
                **stats_dict,
            }
            records.append(record)
    return records


def ensure_channel_first(array: np.ndarray, expected_channels: int) -> np.ndarray:
    arr = np.squeeze(array)
    if arr.ndim == 2:
        arr = arr[np.newaxis, ...]
    if arr.shape[0] == expected_channels:
        return arr
    if arr.ndim == 3 and arr.shape[-1] == expected_channels:
        return np.moveaxis(arr, -1, 0)
    raise ValueError(f"Unable to align channel axis for array with shape {arr.shape}")


def lookup_channel_metadata(
    source_export: str,
    channel_labels: Sequence[str],
    index: int,
    single_lookup: Optional[pd.DataFrame],
) -> ChannelMeta:
    slug = channel_labels[index] if index < len(channel_labels) else f"channel_{index}"
    canonical = slug
    marker = slug
    wavelength: Optional[float] = None

    if single_lookup is not None and source_export in single_lookup.index:
        row = single_lookup.loc[source_export]
        slug = str(row.get("channel_slug") or slug)
        canonical = str(row.get("channel_canonical") or canonical)
        marker = str(row.get("channel_marker") or marker)
        value = row.get("channel_wavelength_nm")
        wavelength = float(value) if pd.notna(value) else None

    return ChannelMeta(slug=slugify(slug), canonical=canonical, marker=marker, wavelength_nm=wavelength)


def compute_statistics(pixels: np.ndarray, mask_pixels: float) -> dict:
    pixel_mean = float(np.mean(pixels))
    pixel_median = float(np.median(pixels))
    pixel_std = float(np.std(pixels, ddof=1)) if pixels.size > 1 else 0.0
    pixel_max = float(np.max(pixels))
    ci_low, ci_high = confidence_interval(pixel_mean, pixel_std, mask_pixels)
    return {
        "mask_pixel_count": float(mask_pixels),
        "pixel_mean": pixel_mean,
        "pixel_median": pixel_median,
        "pixel_std": pixel_std,
        "pixel_max": pixel_max,
        "ci_low": ci_low,
        "ci_high": ci_high,
    }


def confidence_interval(mean: float, std: float, count: float) -> tuple[float, float]:
    if count <= 1 or std == 0.0:
        return (mean, mean)
    standard_error = std / np.sqrt(count)
    margin = CONFIDENCE_Z * standard_error
    return (mean - margin, mean + margin)


def save_channel_outputs(channel_slug: str, df: pd.DataFrame, pipeline_root: Path) -> None:
    channel_dir = pipeline_root / channel_slug
    data_dir = channel_dir / "data"
    figures_dir = channel_dir / "figures"
    data_dir.mkdir(parents=True, exist_ok=True)
    figures_dir.mkdir(parents=True, exist_ok=True)

    df.to_csv(data_dir / "per_image_results.csv", index=False)

    summary = summarise_groups(df)
    if not summary.empty:
        summary.to_csv(data_dir / "group_summary.csv", index=False)

    comparisons = compare_groups(df)
    if comparisons is not None and not comparisons.empty:
        comparisons.to_csv(data_dir / "group_comparisons.csv", index=False)

    for (projection_type), projection_df in df.groupby("projection_type"):
        fig = plot_projection_summary(projection_df, channel_slug, projection_type)
        if fig is None:
            continue
        fig_path = figures_dir / f"{channel_slug}_{projection_type}_pixel_mean_summary.png"
        fig.savefig(fig_path, dpi=300)
        fig.savefig(figures_dir / f"{channel_slug}_{projection_type}_pixel_mean_summary.svg", dpi=300)
        plt.close(fig)


def summarise_groups(df: pd.DataFrame) -> pd.DataFrame:
    summaries: List[dict] = []
    for (projection_type), projection_df in df.groupby("projection_type"):
        for group, group_df in projection_df.groupby("group"):
            pixel_means = group_df["pixel_mean"].astype(float).to_numpy()
            if pixel_means.size == 0:
                continue
            n = pixel_means.size
            std = np.std(pixel_means, ddof=1) if n > 1 else 0.0
            sem = std / np.sqrt(n) if n > 0 else float("nan")
            ci_low, ci_high = confidence_interval(float(np.mean(pixel_means)), std, float(n))
            summaries.append(
                {
                    "projection_type": projection_type,
                    "group": group,
                    "channel_slug": projection_df["channel_slug"].iloc[0],
                    "channel_canonical": projection_df["channel_canonical"].iloc[0],
                    "channel_marker": projection_df["channel_marker"].iloc[0],
                    "channel_wavelength_nm": projection_df["channel_wavelength_nm"].iloc[0],
                    "n": n,
                    "pixel_mean_mean": float(np.mean(pixel_means)),
                    "pixel_mean_median": float(np.median(pixel_means)),
                    "pixel_mean_std": float(std),
                    "pixel_mean_sem": float(sem),
                    "pixel_mean_ci_low": float(ci_low),
                    "pixel_mean_ci_high": float(ci_high),
                }
            )
    return pd.DataFrame(summaries)


def compare_groups(df: pd.DataFrame) -> Optional[pd.DataFrame]:
    comparisons: List[dict] = []
    for projection_type, projection_df in df.groupby("projection_type"):
        wt = projection_df.loc[projection_df["group"] == "WT", "pixel_mean"].astype(float).to_numpy()
        ko = projection_df.loc[projection_df["group"] == "KO", "pixel_mean"].astype(float).to_numpy()
        if wt.size == 0 or ko.size == 0:
            continue
        t_stat, t_p = ttest_ind(wt, ko, equal_var=False)
        u_stat, u_p = mannwhitneyu(wt, ko, alternative="two-sided")
        comparisons.append(
            {
                "projection_type": projection_type,
                "channel_slug": projection_df["channel_slug"].iloc[0],
                "channel_canonical": projection_df["channel_canonical"].iloc[0],
                "channel_marker": projection_df["channel_marker"].iloc[0],
                "t_statistic": float(t_stat),
                "t_pvalue": float(t_p),
                "mannwhitney_statistic": float(u_stat),
                "mannwhitney_pvalue": float(u_p),
                "wt_n": wt.size,
                "ko_n": ko.size,
            }
        )
    if not comparisons:
        return None
    return pd.DataFrame(comparisons)


def plot_projection_summary(projection_df: pd.DataFrame, channel_slug: str, projection_type: str):
    groups = ["WT", "KO"]
    values = [
        projection_df.loc[projection_df["group"] == group, "pixel_mean"].astype(float).to_numpy()
        for group in groups
    ]
    if any(arr.size == 0 for arr in values):
        return None

    fig, axes = plt.subplots(1, 2, figsize=(10, 5), sharey=True)

    # Box plot
    box = axes[0].boxplot(values, labels=groups, patch_artist=True, medianprops={"color": "black"})
    for patch, group in zip(box["boxes"], groups):
        patch.set_facecolor(GROUP_COLORS.get(group, "#999999"))
        patch.set_alpha(0.6)
    axes[0].set_title(f"{projection_type.upper()} projection: pixel mean")
    axes[0].set_ylabel("Pixel mean intensity (a.u.)")
    axes[0].grid(alpha=0.3)

    scatter_x: List[float] = []
    scatter_y: List[float] = []
    for idx, arr in enumerate(values, start=1):
        if arr.size == 0:
            continue
        jitter = np.linspace(-0.12, 0.12, arr.size) if arr.size > 1 else np.array([0.0])
        scatter_x.extend((idx + jitter).tolist())
        scatter_y.extend(arr.tolist())
    if scatter_x:
        axes[0].scatter(scatter_x, scatter_y, color="black", alpha=0.35, s=22, linewidths=0, zorder=3)

    # Bar plot with mean +/- SEM
    means = [float(np.mean(arr)) for arr in values]
    sems = [
        float(np.std(arr, ddof=1) / np.sqrt(arr.size)) if arr.size > 1 else 0.0 for arr in values
    ]
    x = np.arange(len(groups))
    axes[1].bar(
        x,
        means,
        yerr=sems,
        color=[GROUP_COLORS.get(group, "#999999") for group in groups],
        alpha=0.8,
        capsize=8,
    )
    axes[1].set_xticks(x)
    axes[1].set_xticklabels(groups)
    axes[1].set_title("Mean +/- SEM (pixel mean)")
    axes[1].grid(axis="y", alpha=0.3)

    axes[0].set_xlabel("")
    axes[1].set_xlabel("")
    fig.suptitle(
        f"{channel_slug} â€“ WT vs KO pixel mean comparison ({projection_type.upper()} projection)",
        fontsize=14,
    )
    fig.tight_layout()
    return fig


def slugify(value: str) -> str:
    return "".join(ch.lower() if ch.isalnum() else "_" for ch in value).strip("_")


def extract_mask_array(mask_obj: np.ndarray) -> np.ndarray:
    """Return a boolean 2-D mask from a Cellpose *_seg.npy payload."""

    if isinstance(mask_obj, np.ndarray) and mask_obj.dtype == object:
        if mask_obj.shape == ():
            payload = mask_obj.item()
        else:  # list-like of dicts; take first element
            payload = mask_obj.ravel()[0]
        if isinstance(payload, dict):
            masks = payload.get("masks")
            if masks is None:
                raise ValueError("Missing 'masks' entry in Cellpose payload.")
            return np.squeeze(np.asarray(masks)) > 0
    return np.squeeze(np.asarray(mask_obj)) > 0


if __name__ == "__main__":
    main()
